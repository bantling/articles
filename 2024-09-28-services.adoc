// SPDX-License-Identifier: Apache-2.0
:doctype: article

= A simpler alternative to microservices

== Issues a team I used to work with encountered

I worked with a team who decided to use a microservices approach, that worked as follows:

- Chose Go over Java because "Java isn't ready for microservices, as Spring and JPA take over 200MB of memory"
- Use Go net/http for HTTP server with a mux added
- Use Go database/sql with hand written queries, no orm
- A separate git repo for each service
- A separate container for each service
- Postgres database
- Redis to cache query results
- Elasticsearch for full text searching in one of those text/select combo widgets where users can type and see results
  on the fly to select from
- Logstash to populate Elasticsearch
- Websockets to ensure that when multiple users are looking at the same screen and one of them adds an entry to a table,
  the other users see the new table entry show up without clicking anything
- Kafka to create training data and replay later it in a desired amount of clock time, with ability to rewind or
  fast forward
- Swagger to generate a proxy that sits in front of all the services, so the HTML/JS client connect to only one service

Most of these points were not helpful or unnecessary:

- Contrary to popular opinion, Java does not require using Spring or JPA. The JRE comes with HttpHandler and JDBC,
  which are roughly equivalent to the above Go built in packages
- Devs can't be bothered to check out all repos once there are too many to bother with
- Since sometimes service A calls service B, each service needs access to all objects that are converted to/from JSON,
  which leads to a separate repo for object definitions
- A dev working on service B might not have service A checked out. Even if he did, modifying the object definition of B
  won't show a compile error in service A until A is updated with the latest object definitions. This leads to spending
  a lot of time figuring out why occasionally B objects are missing some data.
- Devs were writing code that would check Redis for data, if not found, query Postgres and store it in Redis, despite
  the fact the database would only get about 10,000 rows inserted per year, which is nothing for an SQL database
- Logstash selects data from Postgres to insert into Elasticsearch. If data is deleted in Postgres, you have to find
  some way to select deleted data (eg a trigger that inserts a table name and key into a deletions table)
- Eventually, both Go and Java were used, but Swagger for Go would choke on certain legal constructs due to the Go
  Swagger community being much smaller with fewer resources, requiring a workaround that made things harder in the UI
- Without any real knowledge of service dependencies or tooling, every container would need to be deployed every time
- There was no fault tolerance, if a service goes down that some other service needs, that other service would also fail

Effectively, we had a distributed monolith. A far better, more useful approach would be to simply admit a monolith is
perfectly ok. A monolith doesn't have to mean having only one code base in one language - a hybrid approach can be used
to achieve a monolithic deployment in the sense of always deploying all the code, but still decomposing it into
separate services.

Here is an example of a simpler system, that is more useful in a lot of similar cases:

- Have one repo with top level directories for each service, and one for object definitions
- Use events when requests come in via HTTP or when services communicate to each other. A separate service receives
  these events, and are handled by a simple Map<Event,Processor>. The processor can return a result back to the event
  service, which can pass the result baack to the HTTP client or requesting service. 
- If you have multiple languages, that does not mean you need multiple containers. A single container can run each
  language as a separate process with a separate port. Each language has its own event service. If a service needs to
  call to another language, it can use internal HTTP communication to the other language event service, which works the
  same as if an external client made the same request.
- Create a basic framework for the common case of a data driven service, along the lines of a generic base class with
  methods for upserting, querying, and deleting data from SQL. All a new service needs to do is provide the object type.
- Create an example service that can be copied whenever a new service needs to be created. This example service is not
  included in the runtime process.
- Use a simple build system that compiles all or a chosen subset of service directories. For each service, the compiler
  only sees that one service and the object definitions. This catches cases of a service directly calling the code of
  another service, rather than communicating via events as expected.
- Building a chosen subset of services you want to test locally is useful when you modify the definition of an object
  that causes multiple services to have compile errors. You can choose to test only the owning service first, then test
  dependent services as they are modified to provide the additional information. 
- Redis is pitched as speeding up slow databases. You don't start with a slow database, so don't start with Redis. When
  your database becomes slow, optimize what you can in the database first. For SQL, you have choices like query
  optimizations and restructuring, materialized views, partitions, and replication.
- In this simple usage of Kafka, SQL tables would have been fine instead
- SQL vendors generally provide full text searching. The network round trip to Elasticsearch is not going to be any
  different than the database, and the indexes are not necessarily any faster. Hammering away on SQL queries as the user
  types in a filter box is not autommatically any less performant than hammering away on Elasticsearch queries.
- SQL vendors generally support recursive CTEs, which can  be used for graph queries amongst other things. It is
  possible to use recursive CTEs with one data point, and spider out to the the whole graph containing that point.
- If you need the ability to store arbitrary extra info over and above defined columns (eg, user defined key/value
  pairs), you can use a JSON column to store it.

In general, when you need some new functionality, your first thought should be can I do this with SQL in a reasonable
fashion, and will it work well enough? Try a proof of concept first before adding new infrastructure. More
infrastructure is more complexity, more to deploy, more code, more everything. Don't underestimate how much extra work
is added, and consider that the cost of that extra infrastructure will only grow and become more work over time. Once
infrastructure is added, it tends to hang around for the remainder of the project lifetime.

You can add infrastructure whenever you want, it does not have to be an upfront decision. You can add stuff like Kafka, 
Neo4J, Redis, Logstash, Elasticsearch, DuckDB/Snowflake/etc and anything else as needed. If a solution involving SQL
tables is no longer sufficient, you can migrate to something else, where you have knowledge of a real, rather than
perceived, problem. Consider if the new infrastructure is general enough to use in multiple services, and add it to the
generic base class, to avoid subtle bugs in differences in the usage of it from one service to another.

Another consideration is unknowable future changes, where suddenly you discover that some of the current choies are now
holding you back. An example is companies deciding on a new requirement for vulnerability scans, where your container
cannot contain any critical or high vulnerabilities. If your current approach is something that vacuums up great gobs of
libraries, like Java Spring or Python FastAPI/Uvicorn, then you can find yourself in a situation where you are spending
too much time resolving vuln of the week. You want to deploy, but are held hostage yet again by the latest vuln. Or
maybe you find over time that the library/framework you are using makes breaking changes that you find annoying.

At the same time, the customer also has unknowable changes (row level security), there could be external forces (laws
and regulations), internal forces (make the system less time consuming to develop), that require changes to the base
class(es) and possibly some of the services. If your approach primarily uses a simple event mechanism, with as little
custom code as possible, then it is that much easier to make needed changes.

Any such changes could be applied first to a service that has no custom code to verify it is working, then the service
with the most custom code to ensure the new approach works in the worst case, then the remaining services. You could
even change languages for some services if that is the best path.

With some simple tooling and design choices, you can have a set of services that compose a monolith where you only have
one container for the code, and a container for each piece of infrastructure. The resulting small number of containers
can be run locally, allowing much faster development than having to deploy every change to a development server.
