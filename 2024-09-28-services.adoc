// SPDX-License-Identifier: Apache-2.0
:doctype: article

= A simpler alternative to microservices

== Issues a team I used to work with encountered

I worked with a team who decided to use a microservices approach, that worked as follows:

- Chose Go over Java because "Java isn't ready for microservices, as Spring and JPA take over 200MB of memory"
- Use Go net/http for HTTP server with a mux added
- Use Go database/sql with hand written queries, no orm
- A separate git repo for each service
- A separate container for each service
- Postgres database
- Redis to cache query results
- Elasticsearch for full text searching in one of those text/select combo widgets where users can type and see results
  on the fly to select from
- Logstash to populate Elasticsearch
- Websockets to ensure that when multiple users are looking at the same screen and one of them adds an entry to a table,
  the other users see the new table entry show up without clicking anything
- Kafka to create training data and replay later it in a desired amount of clock time, with ability to rewind or
  fast forward
- Swagger to generate a proxy that sits in front of all the services, so the HTML/JS client connect to only one service

== Most of these decisions were unnecessary, and various problems resulted:

- Contrary to popular opinion, Java does not require using Spring or JPA. The JRE comes with HttpHandler and JDBC,
  which are roughly equivalent to the above Go built in packages
- Devs can't be bothered to check out all repos once there are too many to bother with
- Since sometimes service A calls service B, each service needs access to all objects that are converted to/from JSON,
  which leads to a separate repo for object definitions
- A dev working on service B might not have service A checked out. Even if he did, modifying the object definition of B
  won't show a compile error in service A until A is updated with the latest object definitions. This leads to spending
  a lot of time figuring out why occasionally B objects are missing some data.
- Devs were writing code that would check Redis for data, if not found, query Postgres and store it in Redis, despite
  the fact the database would only get about 10,000 rows inserted per year, which is nothing for an SQL database
- Logstash selects data from Postgres to insert into Elasticsearch. If data is deleted in Postgres, you have to find
  some way to select deleted data (eg a trigger that inserts a table name and key into a deletions table)
- Eventually, both Go and Java were used, but Swagger for Go would choke on certain legal constructs due to the Go
  Swagger community being much smaller with fewer resources, requiring a workaround that made things harder in the UI
- Without any real knowledge of service dependencies or any tooling, every container would need to be deployed every time
- There was no fault tolerance, if a service goes down that some other service needs, that other service would also fail

Effectively, we had what others have called a distributed monolith. A far better, more useful approach would be to
simply admit a monolith is perfectly ok. A monolith doesn't have to mean having only one code base in one language - a
hybrid approach can be used to achieve a monolithic deployment in the sense of always deploying all the code, but still
decomposing it into separate services.

Large companies like a Google or Netflix will have all the tooling they need because they have all the money for
anything they want. This is not true for most teams who want to use some kind of services approach, and I think it would
fair to say were trying to play in the big boys sandbox, without any of the big boys tools.

Here are some ideas for a simpler system, along with some simple tooling, that is more useful for cases where a
monolithic deployment is ok:

- Have one repo with top level directories for each service, and one for object definitions
- Use events when requests come in via HTTP or when services communicate to each other. A separate service receives
  these events, and are handled by a simple Map<Event,Processor>. The processor can return a result back to the event
  service, which can pass the result back to the HTTP client or requesting service.
- Sort out logging details at the outset. If your company uses a system like DataDog to vacuum up all the console logs,
  then only log to the console, no purpose in wasting space on files that will never be read.
- Include a trace id in each request via headers/event data, which can be in the logs. The idea is if Bob hits save on
  Tuesday at 2PM, and you are tracking down the issue, you can start by finding Bob's error, grab the trace id, search
  for that it to get all logs from every service that ultimately was involved in Bob hitting save. Check your logging
  system to see if there is a standard header name you can use that the system recognizes.
- If you have multiple languages, that does not mean you need multiple containers. A single container can run each
  language as a separate process with a separate port. Each language has its own event service. If a service needs to
  call to another language, it can use the event system which makes an internal HTTP call to the other language event
  service. The entry point can be a shell script that executes all but one language in the background, and the
  final language in the foreground. 
- Create a basic framework for the common case of a data driven service, along the lines of a generic base class with
  methods for upserting, querying, and deleting data from SQL. All a new data driven service needs to do to start is
  provide the object type.
- It is ok to use something like Jave JPA or Spring that uses a lot of memory, because you only have one copy of it for
  all services.
- Create an example service that can be copied whenever a new service needs to be created. This example service is not
  included in the runtime process. You could have one of these for each language, in a separate top level directory.
- Compile all or a chosen subset of service directories. For each service, the compiler only sees that one service and
  the object definitions. This catches cases of a service directly calling the code of another service, rather than
  communicating via events as expected.
- Build a chosen subset of services you want to test locally, so that when you modify the definition of an object that
  causes multiple services to have compile errors, you can choose to test only the owning service first, then test
  dependent services as they are modified to provide the additional information. 
- Redis is pitched as speeding up slow databases. You don't start with a slow database, so don't start with Redis. When
  your database becomes slow, optimize what you can in the database first. For SQL, you have choices like query
  optimizations and restructuring, materialized views, partitions, and replication.
- In this simple usage of Kafka, SQL tables probably would have been fine instead
- SQL vendors generally provide full text searching. The network round trip to Elasticsearch is not going to be any
  different than the database, and the indexes are not necessarily any faster. Hammering away on SQL queries as the user
  types in a filter box is not automatically any less performant than hammering away on Elasticsearch queries.
- SQL vendors generally support recursive CTEs, which can  be used for graph queries amongst other things. It is
  possible to use recursive CTEs with one data point, and spider out to the the whole graph containing that point.
- If you need the ability to store arbitrary extra info over and above defined columns (eg, user defined key/value
  pairs), you can use a JSON column to store it.
- Performance is a problem when the customer says it is a problem, not until. A real performance problem has a
  measurement, so that a measurable gain can be provided when the performance has been improved.

In general, when you need some new functionality, your first thought should be can I do this with SQL in a reasonable
fashion, and will it work well enough? Try a proof of concept first before adding new infrastructure. More
infrastructure is more complexity, more to deploy, more to code, more everything. Don't underestimate how much extra
work is added, and consider that the cost of that extra infrastructure will only grow and become more work over time.
Once infrastructure is added, it tends to hang around for the remainder of the project lifetime.

You can add infrastructure whenever you want, it does not have to be an upfront decision. You can add stuff like Kafka, 
Neo4J, Redis, Logstash, Elasticsearch, DuckDB/Snowflake/etc as needed. If a solution involving SQL tables is no longer
sufficient, you can migrate to something else, where you have knowledge of a real, rather than perceived, problem.
Consider if the new infrastructure is general enough to use in multiple services, and add it to the generic base class,
to avoid subtle bugs in differences in the usage of it from one service to another.

You will get unknowable future changes, where suddenly you discover that some of the current choices are now holding you
back. An example is a company decision on a new requirement for vulnerability scans, where your container cannot contain
any critical or high vulnerabilities. If your current approach is something that vacuums up great gobs of libraries,
like Java Spring or Python FastAPI/Uvicorn, then you can find yourself in a situation where you are spending too much
time resolving vuln of the week. You want to deploy, but are held hostage yet again by the latest vuln. Or maybe you
find over time that the library/framework you are using makes breaking changes that you find annoying.

At the same time, the customer also has unknowable changes that can impact the design on some level, there could be
external forces (laws and regulations), internal forces (make the system less time consuming to develop), that require
changes to the base class(es) and possibly some of the services. If your approach primarily uses a simple event
mechanism, with base classes that require as little custom code as possible, then it is that much easier to make needed
changes.

Any such changes could be applied first to a service that has no custom code to verify it is working, then the service
with the most custom code to ensure the new approach works in the worst case, then the remaining services. You could
change languages for some services if that is the best path, or even leave some as is.

== Conclusion

With some simple tooling for compiling what you want, running it locally in containers, communicating by events, you can
have a set of services that compose a monolith where you only have one container for the code, and a container for each
piece of infrastructure. The resulting small number of containers can generally be run locally without issue, allowing
much faster development than having to deploy every change to a development server.
