// SPDX-License-Identifier: Apache-2.0
:doctype: article

= A simpler alternative to microservices

== Issues a team I used to work with encountered

I worked with a team who decided to use a microservices approach, that worked as follows:

- Chose Go over Java because "Java isn't ready for microservices, as Spring and JPA take over 200MB of memory"
- Use Go net/http for HTTP server with a mux added
- Use Go database/sql with hand written queries, no orm
- A separate git repo for each service
- A separate container for each service
- Postgres database
- Redis to cache query results
- Elasticsearch for full text searching in one of those text/select combo widgets where users can type and see results
  on the fly to select from
- Logstash to populate Elasticsearch
- Websockets to ensure that when multiple users are looking at the same screen and one of them adds an entry to a table,
  the other users see the new table entry show up without clicking anything
- Kafka to create training data and replay later it in a desired amount of clock time, with ability to rewind or
  fast forward
- Swagger to generate a proxy that sits in front of all the services, so the HTML/JS client connect to only one service

== Most of these decisions were unnecessary, and various problems resulted:

- Contrary to popular opinion, Java does not require using Spring or JPA. The JRE comes with HttpHandler and JDBC,
  which are roughly equivalent to the above Go built in packages
- Devs can't be bothered to check out all repos once there are too many to bother with
- Since sometimes service A calls service B, each service needs access to all objects that are converted to/from JSON,
  which leads to a separate repo for object definitions
- A dev working on service B might not have service A checked out. Even if he did, modifying the object definition of B
  won't show a compile error in service A until A is updated with the latest object definitions. This leads to spending
  a lot of time figuring out why occasionally B objects are missing some data.
- Devs were writing code that would check Redis for data, if not found, query Postgres and store it in Redis, despite
  the fact the database would only get about 10,000 rows inserted per year, which is nothing for an SQL database
- Logstash selects data from Postgres to insert into Elasticsearch. If data is deleted in Postgres, you have to find
  some way to select deleted data (eg a trigger that inserts a table name and key into a deletions table)
- Eventually, both Go and Java were used, but Swagger for Go would choke on certain legal constructs due to the Go
  Swagger community being much smaller with fewer resources, requiring a workaround that made things harder in the UI
- Without any real knowledge of service dependencies or any tooling, every container would need to be deployed every time
- There was no fault tolerance, if a service goes down that some other service needs, that other service would also fail

Effectively, we had what others have called a distributed monolith. It really wasn't microservices, it was but a poor
mans shadow of microservices. A far better, more useful approach would be to simply admit a monolith is perfectly ok.
A monolith doesn't have to mean having only one code base in one language - a hybrid approach can be used to achieve a
monolithic deployment in the sense of always deploying all the code, but still decomposing it into separate services.

Large companies like a Google or Netflix will have all the tooling they need because they have all the money for
anything they want. This is not true for most teams who want to use some kind of services approach, and I think it would
fair to say in the above example, we were trying to play in the big boys sandbox, without any of the big boys tools.

I don't see anything wrong with have a distributed monolith - microservices it not for everyone, it is no more a saviour
of developer kind than any other approach we have taken over the years. Not everyone needs to have things like fault
tolerance, where if service A calls service B and service B is down, service A can continue to work in a degraded
fashion, rather than erroring out.

Here are some ideas for a simpler system, along with some simple tooling, that is more useful for cases where a
monolithic deployment is ok:

- Have one repo with top level directories for each language, that contain subdirectories for objects, config, events,
  example service, and services
- Use events when requests come in via HTTP or when services communicate to each other. The events are handled by a
  simple Map<Event,Processor>. The processor receives data from the event (eg URL path variables, query params, cookies,
  the decoded request body as the appropriate object, etc), and can return a result back to the event service, which can
  pass the result back to the HTTP client or requesting service.
- If languages need to talk to each other, they can do so by hitting the event service of the other language. Eg, the
  user sends an HTTP request to the Go event service, that dispatches to a particular Go service, that sends an HTTP
  request to the Java event service, that dispatchees to a particular Java service, and the result makes its way back
  to the Go service, and ultimately back to the initial HTTP response.
- Sort out logging details at the outset. If your company uses a system like DataDog to vacuum up all the console logs,
  then only log to the console, no purpose in wasting space on files that will never be read.
- Include a trace id in each request via headers/event data, which is provided in the logs. The idea is if Bob hits save
  on Tuesday at 2PM, and you are tracking down the issue, you can start by finding Bob's error, grab the trace id,
  search for it to get all logs from every service that ultimately was involved in Bob hitting save. Check your log
  viewing/analyzing system to see if there is a well known header name you can use that the system recognizes.
- If you have multiple languages, they are separate containers
- You can use podman to run all the code containers and infrastructure containers locally in a single pod, kind of like
  docker composse, but simpler. The containers are on the same network, so can contact each other as localhost:port.  
- Create a basic framework for the common case of a data driven service, along the lines of a generic base class with
  methods for upserting, querying, and deleting data from SQL. All a new data driven service needs to do is provide the
  object type. Consider using PUT as upsert as I wrote in another post about using more RFC compliant REST APIs for
  simplicity (https://www.tumblr.com/greggermeister/761096020234043392/rest-and-http-semantics), and leaving POST
  available for non-crud operations.
- It is ok to use something like Jave JPA or Spring that uses a lot of memory, because there will only be one container
  with all Java servicces, therefore you only have only copy of Spring and JPA in memory.
- Create an example service in each language that can be copied whenever a new service needs to be created. This example
  service is not included in the runtime process.
- Compile all or a chosen subset of service directories. For each service, the compiler only sees that one service and
  the object definitions. This catches cases of a service directly calling the code of another service, rather than
  communicating via events as expected.
- Build a chosen subset of services you want to test locally, so that when you modify the definition of an object that
  causes multiple services to have compile errors, you can choose to test only the owning service first, then test
  dependent services as they are modified to provide the additional information. 
- Redis is pitched as speeding up slow databases. You don't start with a slow database, so don't start with Redis. When
  your database becomes slow, optimize what you can in the database first. For SQL, you have choices like query
  optimizations and restructuring, materialized views, partitions, and replication.
- In this simple usage of Kafka, SQL tables probably would have been fine instead
- SQL vendors generally provide full text searching. The network round trip to Elasticsearch is not going to be any
  different than the database, and the indexes are not necessarily any faster. Hammering away on SQL queries as the user
  types in a filter box is not automatically any less performant than hammering away on Elasticsearch queries.
- SQL vendors generally support recursive CTEs, which can  be used for graph queries amongst other things. It is
  possible to use recursive CTEs with one data point, and spider out to the the whole graph containing that point.
- If you need the ability to store arbitrary extra info over and above defined columns (eg, user defined key/value
  pairs), you can use a JSON column to store it.
- A real performance problem has a measurement, so that a measurable gain can be provided when the performance has been
  improved. Ideally, metrics should be defined and monitored, so that when a threshold like 80% of a given limit has
  been reached, devs can consider how to improve that metric before customers complain about it. Look into the details
  of the log viewing/analyzing system, it may have the features you need.

In general, when you need some new functionality, your first thought should be can I do this with SQL in a reasonable
fashion, and will it work well enough? Try a proof of concept first before adding new infrastructure. More
infrastructure is more complexity, more to deploy, more to code, more everything. Don't underestimate how much extra
work is added, and consider that the cost of that extra infrastructure will only grow and become more work over time.
Once infrastructure is added, it tends to hang around for the remainder of the project lifetime.

You can add infrastructure whenever you want, it does not have to be an upfront decision. You can add stuff like Kafka, 
Neo4J, Redis, Logstash, Elasticsearch, DuckDB/Snowflake/BigQuery, etc as needed. If a solution involving SQL tables is
no longer sufficient, you can migrate to something else, where you have knowledge of a real, rather than perceived,
problem. Consider if the new infrastructure is general enough to use in multiple services, and add it to the generic
base class, to avoid subtle bugs in differences in the usage of it from one service to another.

You will get unknowable future changes, where suddenly you discover that some of the current choices are now holding you
back. An example is a company decision on a new requirement for vulnerability scans, where your container cannot contain
any critical or high vulnerabilities. If your current approach is something that vacuums up great gobs of libraries,
like Java Spring or Python FastAPI/Uvicorn, then you can find yourself in a situation where you are spending too much
time resolving vuln of the week. You want to deploy, but are held hostage yet again by the latest vuln. Or maybe you
find over time that the library/framework you are using makes breaking changes that you find annoying.

At the same time, the customer also has unknowable changes that can impact the design on some level. There could also be
external forces (laws and regulations), internal forces (make the system less time consuming to develop). All of these
forces can require changes to the base class(es) and possibly some of the services. If your approach primarily uses a
simple event mechanism, with base classes that require as little custom code as possible, then it is that much easier to
make needed changes.

Any such changes could be applied first to a service that has no custom code to verify it is working, then the service
with the most custom code to ensure the new approach works in the worst case, then the remaining services. You could
change languages for some services if that is the best path, or even leave some as is.

== Conclusion

With some simple tooling for compiling what you want, running it locally in containers, communicating by events, with a
logging and monitoring strategy, you can have a set of services that compose a monolith where every service is deployed
every time, and you only have one container for each language. The resulting small number of containers can generally be
run locally without issue, allowing much faster development than having to deploy every change to a development server.

The net effect is not a full blown microservices platform, but a more realistic take on deploying services when your
company isn't big enough to make the kind of tooling required for full microservices, but you get the basic benefits.
